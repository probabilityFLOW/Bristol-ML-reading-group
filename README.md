# Bristol-ML-reading-group

This website collects papers read in the Bristol Machine Learning Reading Group.

## 2025

| Title | Date | Topic |
|:-------|:------:|:-------:|
| [Guiding a Diffusion Model with a Bad Version of Itself](https://arxiv.org/abs/2406.02507) | 9 JAN | Diffusion |
| [Accelerated Diffusion Models via Speculative Sampling](https://arxiv.org/abs/2501.05370)<br>[SpecTr: Fast Speculative Decoding via Optimal Transport](https://arxiv.org/abs/2310.15141) | 21 JAN | Speculative Sampling |
|[Test-time regression: a unifying framework for designing sequence models with associative memory](https://arxiv.org/abs/2501.12352)|23 JAN | Sequence Model|
|[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)| 28 JAN | LLM |
|[Consistency Models](https://arxiv.org/abs/2303.01469)<br>[SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations](https://arxiv.org/abs/2502.02472)| 6 FEB | Diffusion |
| [Test-Time Training with Self-Supervision for Generalization under Distribution Shifts](https://proceedings.mlr.press/v119/sun20b.html) | 11 FEB | Test-time Training |
| [Measure-to-measure interpolation using Transformers](https://arxiv.org/abs/2411.04551) | 4 MAR | Transformer |
|[Simplified and Generalized Masked Diffusion for Discrete Data](https://arxiv.org/abs/2406.04329)| 13 MAR | Diffusion |
|[Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms](https://arxiv.org/abs/2503.07154)<br>[Inductive Moment Matching](https://arxiv.org/abs/2503.07565)| 18 MAR | Scaling in Diffusion |
|[Towards a Mechanistic Explanation of Diffusion Model Generalization](https://arxiv.org/abs/2411.19339)| 20 MAR | Diffusion |
|[A Statistical Theory of Contrastive Learning via Approximate Sufficient Statistics](https://arxiv.org/abs/2503.17538)<br>[A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)| 25 MAR | Contrastive Learning |
|[Approximate Bayesian Computation with Deep Learning and Conformal prediction](https://arxiv.org/abs/2406.04874) | 27 MAR | ABC |
|[NoProp: Training Neural Networks without Back-propagation or Forward-propagation](https://arxiv.org/abs/2503.24322)| 1 APR | NN |
|[One Step Diffusion via Shortcut Models](https://arxiv.org/abs/2410.12557)| 3 APR| Diffusion |
|[Normalization Techniques in Training DNNs: Methodology, Analysis and Application](https://arxiv.org/abs/2009.12836)<br>[Layer Normalization](https://arxiv.org/abs/1607.06450)| 8 APR | LN |
|[The Mathematics of Artificial Intelligence](https://arxiv.org/abs/2501.10465)| 10 APR | Overview |
|[Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the Wasserstein Geometry](https://arxiv.org/abs/2410.03282)| 1 MAY | Sampling |
|[LEAPS: A discrete neural sampler via locally equivariant networks](https://arxiv.org/abs/2502.10843)| 8 MAY | Sampling |
| Same as 8 MAY | 13 MAY | Sampling |
| [Reconstructing Training Data from Trained Neural Networks](https://arxiv.org/abs/2206.07758)<br>[Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization](https://arxiv.org/abs/2408.02584)| 20 MAY | NN/LLM |
|[Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training](https://arxiv.org/abs/2505.17638)| 27 MAY | Diffusion |
|[Joint Learning in the Gaussian Single Index Model](https://arxiv.org/abs/2505.21336)| 3 JUN | Representation learning |
|[Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)| 5 JUN | Diffusion |
|[A Generalization Result for Convergence in Learning-to-Optimize](https://arxiv.org/abs/2410.07704)| 10 JUN | Learning theory |
|[Neural Adaptive Sequential Monte Carlo](https://arxiv.org/abs/1506.03338)<br>[Alternators For Sequence Modeling](https://arxiv.org/abs/2405.11848)| 12 JUN | Sampling |
|[Composition and Control with Distilled Energy Diffusion Models and Sequential Monte Carlo](https://arxiv.org/abs/2502.12786)| 17 JUN | Diffusion |
|[On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity](https://www.arxiv.org/abs/2506.03719)| 19 JUN | Flow |

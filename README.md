# Bristol-ML-reading-group

This website collects papers read in the Bristol Machine Learning Reading Group.

## Years
- [2025](#2025)
- [2026](#2026)

## Papers
### 2025

| Title | Date | Topic |
|:-------|:------:|:-------:|
| [Guiding a Diffusion Model with a Bad Version of Itself](https://arxiv.org/abs/2406.02507) | 9 JAN | Diffusion |
| [Accelerated Diffusion Models via Speculative Sampling](https://arxiv.org/abs/2501.05370)<br>[SpecTr: Fast Speculative Decoding via Optimal Transport](https://arxiv.org/abs/2310.15141) | 21 JAN | Speculative Sampling |
|[Test-time regression: a unifying framework for designing sequence models with associative memory](https://arxiv.org/abs/2501.12352)|23 JAN | Sequence Model|
|[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)| 28 JAN | LLM |
|[Consistency Models](https://arxiv.org/abs/2303.01469)<br>[SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations](https://arxiv.org/abs/2502.02472)| 6 FEB | Diffusion |
| [Test-Time Training with Self-Supervision for Generalization under Distribution Shifts](https://proceedings.mlr.press/v119/sun20b.html) | 11 FEB | Test-time Training |
| [Measure-to-measure interpolation using Transformers](https://arxiv.org/abs/2411.04551) | 4 MAR | Transformer |
|[Simplified and Generalized Masked Diffusion for Discrete Data](https://arxiv.org/abs/2406.04329)| 13 MAR | Diffusion |
|[Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms](https://arxiv.org/abs/2503.07154)<br>[Inductive Moment Matching](https://arxiv.org/abs/2503.07565)| 18 MAR | Scaling in Diffusion |
|[Towards a Mechanistic Explanation of Diffusion Model Generalization](https://arxiv.org/abs/2411.19339)| 20 MAR | Diffusion |
|[A Statistical Theory of Contrastive Learning via Approximate Sufficient Statistics](https://arxiv.org/abs/2503.17538)<br>[A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)| 25 MAR | Contrastive Learning |
|[Approximate Bayesian Computation with Deep Learning and Conformal prediction](https://arxiv.org/abs/2406.04874) | 27 MAR | ABC |
|[NoProp: Training Neural Networks without Back-propagation or Forward-propagation](https://arxiv.org/abs/2503.24322)| 1 APR | NN |
|[One Step Diffusion via Shortcut Models](https://arxiv.org/abs/2410.12557)| 3 APR| Diffusion |
|[Normalization Techniques in Training DNNs: Methodology, Analysis and Application](https://arxiv.org/abs/2009.12836)<br>[Layer Normalization](https://arxiv.org/abs/1607.06450)| 8 APR | LN |
|[The Mathematics of Artificial Intelligence](https://arxiv.org/abs/2501.10465)| 10 APR | Overview |
|[Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the Wasserstein Geometry](https://arxiv.org/abs/2410.03282)| 1 MAY | Sampling |
|[LEAPS: A discrete neural sampler via locally equivariant networks](https://arxiv.org/abs/2502.10843)| 8 MAY | Sampling |
| Same as 8 MAY | 13 MAY | Sampling |
| [Reconstructing Training Data from Trained Neural Networks](https://arxiv.org/abs/2206.07758)<br>[Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization](https://arxiv.org/abs/2408.02584)| 20 MAY | NN/LLM |
|[Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training](https://arxiv.org/abs/2505.17638)| 27 MAY | Diffusion |
|[Joint Learning in the Gaussian Single Index Model](https://arxiv.org/abs/2505.21336)| 3 JUN | Representation learning |
|[Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)| 5 JUN | Diffusion |
|[A Generalization Result for Convergence in Learning-to-Optimize](https://arxiv.org/abs/2410.07704)| 10 JUN | Learning theory |
|[Neural Adaptive Sequential Monte Carlo](https://arxiv.org/abs/1506.03338)<br>[Alternators For Sequence Modeling](https://arxiv.org/abs/2405.11848)| 12 JUN | Sampling |
|[Composition and Control with Distilled Energy Diffusion Models and Sequential Monte Carlo](https://arxiv.org/abs/2502.12786)| 17 JUN | Diffusion |
|[On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity](https://www.arxiv.org/abs/2506.03719)| 19 JUN | Flow |
|[Temporal Difference Flows](https://arxiv.org/abs/2503.09817)| 24 JUN | FLOW & RL |
|[Do Bayesian Neural Networks Need To Be Fully Stochastic?](https://arxiv.org/abs/2211.06291)| 26 JUN | NN |
|[Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey and New Connections](https://arxiv.org/abs/2404.02954)| 1 JUL | GM |
| same as last | 3 JUL | GM |
|[Stable generative modeling using Schrödinger bridges](https://arxiv.org/abs/2401.04372)| 29 JUL | GM |
|[Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)| 31 JUL | Flow |
|[Optimality of empirical measures as quantizers](https://arxiv.org/abs/2508.02615)| 7 AUG | Learning Theory |
|[Convergence of Deterministic and Stochastic Diffusion-Model Samplers: A Simple Analysis in Wasserstein Distance](https://www.arxiv.org/abs/2508.03210)| 12 AUG | Learning Theory |
|[Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent](https://arxiv.org/abs/2508.08222)| 19 AUG | Learning Theory |
|[Position: Transformers Have the Potential to Achieve AGI](https://openreview.net/forum?id=vMTijVnXQ8)| 21 AUG | Theory |
|[Solving Empirical Bayes via Transformers](https://arxiv.org/abs/2502.09844)| 28 AUG | Bayes |
|[Wild refitting for black box prediction](https://arxiv.org/abs/2506.21460)| 2 SEP | Learning Theory |
|[Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)| 4 SEP | Sampling |
|[Statistical exploration of the Manifold Hypothesis](https://arxiv.org/abs/2208.11665)| 11 SEP | Learning Theory |
| same as last | 16 SEP | Learning Theory |
|[The Broader Landscape of Robustness in Algorithmic Statistics](https://arxiv.org/abs/2412.02670)| 18 SEP | Learning Theory |
|[What is the long-run distribution of stochastic gradient descent? A large deviations analysis](https://arxiv.org/abs/2406.09241)|23 SEP | Learning Theory |
|[Least squares variational inference](https://arxiv.org/abs/2502.18475)| 25 SEP | Generative Model |
|[Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law](https://arxiv.org/abs/2505.19227)| 30 SEP | Learning Theory |
|[High-Dimensional Analysis of Single-Layer Attention for Sparse-Token Classification](https://arxiv.org/abs/2509.25153)| 02 OCT | Learning Theory |
|[Understanding Optimization in Deep Learning with Central Flows](https://arxiv.org/abs/2410.24206)| 07 OCT | Learning Theory |
|[Learning threshold neurons via the "edge of stability"](https://arxiv.org/abs/2212.07469)| 09 OCT | Learning Theory |
|[Variational Learning Finds Flatter Solutions at the Edge of Stability](https://arxiv.org/abs/2506.12903)| 14 OCT | Learning Theory |
|[Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression](https://arxiv.org/abs/2506.02336)| 16 OCT | Learning Theory |
|[Essentially No Barriers in Neural Network Energy Landscape](https://arxiv.org/abs/1803.00885)| 21 OCT | Learning Theory |
|[A Geometric Analysis of PCA](https://arxiv.org/abs/2510.20978v1)| 28 OCT | PCA |
|[Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations](https://arxiv.org/abs/2510.25544)| 30 OCT | Diffusion |
|[Every Language Model Has a Forgery-Resistant Signature](https://arxiv.org/abs/2510.14086v1)| 4 NOV | LLM |
|[Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review](https://arxiv.org/abs/2407.13734)| 06 NOV | RL |
| same as last | 11 NOV | RL |
|[Fast and accurate optimization on the orthogonal manifold without retraction](https://arxiv.org/abs/2102.07432)| 13 NOV | Optimisation |
|[Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits and Deletions](https://arxiv.org/abs/2511.09465)| 18 NOV | GM |
|[Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts](https://arxiv.org/abs/2503.02819)| 20 NOV | GM |
|[Nested learning](https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/)| 25 NOV | Learning Theory |
|[Learn to Guide Your Diffusion Model](https://arxiv.org/abs/2510.00815)| 27 NOV | GM |
|[Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/abs/2510.00815)| 2 DEC | Attention |
|[Convergence of Deterministic and Stochastic Diffusion-Model Samplers: A Simple Analysis in Wasserstein Distance](https://arxiv.org/abs/2508.03210)| 4 DEC | LT |
|[Mean Flows for One-step Generative Modeling](https://arxiv.org/abs/2505.13447)| 12 DEC | GM |
|[Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models](https://arxiv.org/abs/2512.02636)| 16 DEC | GM |
|[Localized Diffusion Models](https://arxiv.org/abs/2505.04417)| 18 DEC | Diffusion |

### 2026

| Title | Date | Topic |
|:-------|:------:|:-------:|
| [Likelihood-Preserving Embeddings for Statistical Inference](https://arxiv.org/abs/2512.22638) | 12 JAN | Representation |
|[Score-based sampling without diffusions: Guidance from a simple and modular scheme](https://arxiv.org/abs/2512.24152)| 15 JAN | Theory |
|[Learning the score under shape constraints](https://arxiv.org/abs/2512.14624)| 19 JAN | Theory |
|[Conditioning diffusion models by explicit forward-backward bridging](https://arxiv.org/abs/2405.13794)<br>[Generative modelling meets Bayesian inference: a new paradigm for inverse problems](https://royalsocietypublishing.org/rsta/article/383/2299/20240334/234777/Generative-modelling-meets-Bayesian-inference-a) | 22 JAN | Diffusion |
|[Likelihood Ratio Tests by Kernel Gaussian Embedding](https://arxiv.org/abs/2508.07982)| 26 JAN | Kernel |
|[Discrete Feynman-Kac Correctors](https://arxiv.org/abs/2601.10403)| 29 JAN | Diffusion |
|[A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo](https://arxiv.org/abs/2510.07559)| 2 FEB | Theory |
|[Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability](https://arxiv.org/abs/2602.01480)| 5 FEB | Learning Theory |
|[High-accuracy sampling for diffusion models and log-concave distributions](https://arxiv.org/abs/2602.01338)| 9 FEB | Diffusion |
|[Step-Size Stability in Stochastic Optimization: A Theoretical Perspective](https://arxiv.org/abs/2602.09842)| 12 FEB | Learning Theory |
|[µpscaling Small Models: Principled Warm Starts and Hyperparameter Transfer](https://arxiv.org/pdf/2602.10545)| 16 FEB | Learning Theory |
|[Universal priors: solving empirical Bayes via Bayesian inference and pretraining](https://arxiv.org/abs/2602.15136v1)| 23 FEB | Bayes |
|[Demystifying Chains, Trees, and Graphs of Thoughts](https://arxiv.org/html/2401.14295v3)| 26 FEB | LLM |






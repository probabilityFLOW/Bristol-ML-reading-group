# Bristol-ML-reading-group

This website collects papers read in the Bristol Machine Learning Reading Group.

## 2025

| Title | Date | Topic |
|:-------|:------:|:-------:|
| [Guiding a Diffusion Model with a Bad Version of Itself](https://arxiv.org/abs/2406.02507) | 9 JAN | Diffusion |
| [Accelerated Diffusion Models via Speculative Sampling](https://arxiv.org/abs/2501.05370)<br>[SpecTr: Fast Speculative Decoding via Optimal Transport](https://arxiv.org/abs/2310.15141) | 21 JAN | Speculative Sampling |
|[Test-time regression: a unifying framework for designing sequence models with associative memory](https://arxiv.org/abs/2501.12352)|23 JAN | Sequence Model|
|[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)| 28 JAN | LLM |
|[Consistency Models](https://arxiv.org/abs/2303.01469)<br>[SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations](https://arxiv.org/abs/2502.02472)| 6 FEB | Diffusion |
| [Test-Time Training with Self-Supervision for Generalization under Distribution Shifts](https://proceedings.mlr.press/v119/sun20b.html) | 11 FEB | Test-time Training |
| [Measure-to-measure interpolation using Transformers](https://arxiv.org/abs/2411.04551) | 4 MAR | Transformer |
|[Simplified and Generalized Masked Diffusion for Discrete Data](https://arxiv.org/abs/2406.04329)| 13 MAR | Diffusion |
|[Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms](https://arxiv.org/abs/2503.07154)<br>[Inductive Moment Matching](https://arxiv.org/abs/2503.07565)| 18 MAR | Scaling in Diffusion |
|[Towards a Mechanistic Explanation of Diffusion Model Generalization](https://arxiv.org/abs/2411.19339)| 20 MAR | Diffusion |
|[A Statistical Theory of Contrastive Learning via Approximate Sufficient Statistics](https://arxiv.org/abs/2503.17538)<br>[A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)| 25 MAR | Contrastive Learning |
|[Approximate Bayesian Computation with Deep Learning and Conformal prediction](https://arxiv.org/abs/2406.04874) | 27 MAR | ABC |
|[NoProp: Training Neural Networks without Back-propagation or Forward-propagation](https://arxiv.org/abs/2503.24322)| 1 APR | NN |
|[One Step Diffusion via Shortcut Models](https://arxiv.org/abs/2410.12557)| 3 APR| Diffusion |
|[Normalization Techniques in Training DNNs: Methodology, Analysis and Application](https://arxiv.org/abs/2009.12836)<br>[Layer Normalization](https://arxiv.org/abs/1607.06450)| 8 APR | LN |
|[The Mathematics of Artificial Intelligence](https://arxiv.org/abs/2501.10465)| 10 APR | Overview |
|[Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the Wasserstein Geometry](https://arxiv.org/abs/2410.03282)| 1 MAY | Sampling |
|[LEAPS: A discrete neural sampler via locally equivariant networks](https://arxiv.org/abs/2502.10843)| 8 MAY | Sampling |
| Same as 8 MAY | 13 MAY | Sampling |
| [Reconstructing Training Data from Trained Neural Networks](https://arxiv.org/abs/2206.07758)<br>[Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization](https://arxiv.org/abs/2408.02584)| 20 MAY | NN/LLM |
|[Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training](https://arxiv.org/abs/2505.17638)| 27 MAY | Diffusion |
|[Joint Learning in the Gaussian Single Index Model](https://arxiv.org/abs/2505.21336)| 3 JUN | Representation learning |
|[Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)| 5 JUN | Diffusion |
|[A Generalization Result for Convergence in Learning-to-Optimize](https://arxiv.org/abs/2410.07704)| 10 JUN | Learning theory |
|[Neural Adaptive Sequential Monte Carlo](https://arxiv.org/abs/1506.03338)<br>[Alternators For Sequence Modeling](https://arxiv.org/abs/2405.11848)| 12 JUN | Sampling |
|[Composition and Control with Distilled Energy Diffusion Models and Sequential Monte Carlo](https://arxiv.org/abs/2502.12786)| 17 JUN | Diffusion |
|[On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity](https://www.arxiv.org/abs/2506.03719)| 19 JUN | Flow |
|[Temporal Difference Flows](https://arxiv.org/abs/2503.09817)| 24 JUN | FLOW & RL |
|[Do Bayesian Neural Networks Need To Be Fully Stochastic?](https://arxiv.org/abs/2211.06291)| 26 JUN | NN |
|[Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey and New Connections](https://arxiv.org/abs/2404.02954)| 1 JUL | GM |
| same as last | 3 JUL | GM |
|[Stable generative modeling using Schr√∂dinger bridges](https://arxiv.org/abs/2401.04372)| 29 JUL | GM |
|[Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)| 31 JUL | Flow |
|[Optimality of empirical measures as quantizers](https://arxiv.org/abs/2508.02615)| 7 AUG | Learning Theory |
|[Convergence of Deterministic and Stochastic Diffusion-Model Samplers: A Simple Analysis in Wasserstein Distance](https://www.arxiv.org/abs/2508.03210)| 12 AUG | Learning Theory |
|[Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent](https://arxiv.org/abs/2508.08222)| 19 AUG | Learning Theory |
|[Position: Transformers Have the Potential to Achieve AGI](https://openreview.net/forum?id=vMTijVnXQ8)| 21 AUG | Theory |
|[Solving Empirical Bayes via Transformers](https://arxiv.org/abs/2502.09844)| 28 AUG | Bayes |
|[Wild refitting for black box prediction](https://arxiv.org/abs/2506.21460)| 2 SEP | Learning Theory |
|[Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)| 4 SEP | Sampling |
|[Statistical exploration of the Manifold Hypothesis](https://arxiv.org/abs/2208.11665)| 11 SEP | Learning Theory |
| same as last | 16 SEP | Learning Theory |
|[The Broader Landscape of Robustness in Algorithmic Statistics](https://arxiv.org/abs/2412.02670)| 18 SEP | Learning Theory |
|[What is the long-run distribution of stochastic gradient descent? A large deviations analysis](https://arxiv.org/abs/2406.09241)|23 SEP | Learning Theory |
